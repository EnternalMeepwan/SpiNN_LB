{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import collections\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import random\n",
    "random.seed(100)\n",
    "runtime = 12000\n",
    "time_step = 1000\n",
    "time_scale_factor = 5\n",
    "cores_per_chip = 10\n",
    "\n",
    "max_offset = 1500\n",
    "def generate_offset(processor):\n",
    "    return min(math.ceil((processor - random.random()) / (cores_per_chip+4) * max_offset), max_offset)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Copyright (c) 2017-2019 The University of Manchester\n",
    "#\n",
    "# This program is free software: you can redistribute it and/or modify\n",
    "# it under the terms of the GNU General Public License as published by\n",
    "# the Free Software Foundation, either version 3 of the License, or\n",
    "# (at your option) any later version.\n",
    "#\n",
    "# This program is distributed in the hope that it will be useful,\n",
    "# but WITHOUT ANY WARRANTY; without even the implied warranty of\n",
    "# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the\n",
    "# GNU General Public License for more details.\n",
    "#\n",
    "# You should have received a copy of the GNU General Public License\n",
    "# along with this program.  If not, see <http://www.gnu.org/licenses/>.\n",
    "from collections import OrderedDict\n",
    "from enum import Enum\n",
    "import struct\n",
    "\n",
    "from spinn_front_end_common.abstract_models import AbstractProvidesNKeysForPartition\n",
    "from spinn_utilities.overrides import overrides\n",
    "from pacman.executor.injection_decorator import inject_items\n",
    "from pacman.model.graphs.machine import MachineVertex\n",
    "from pacman.model.resources import ResourceContainer, VariableSDRAM\n",
    "from pacman.utilities.utility_calls import is_single\n",
    "from spinn_front_end_common.utilities.constants import (\n",
    "    SYSTEM_BYTES_REQUIREMENT, BYTES_PER_WORD)\n",
    "from spinn_front_end_common.utilities.exceptions import ConfigurationException\n",
    "from spinn_front_end_common.utilities.helpful_functions import (\n",
    "    locate_memory_region_for_placement)\n",
    "from spinn_front_end_common.abstract_models.impl import (\n",
    "    MachineDataSpecableVertex)\n",
    "from spinn_front_end_common.interface.buffer_management.buffer_models import (\n",
    "    AbstractReceiveBuffersToHost)\n",
    "from spinn_front_end_common.interface.buffer_management import (\n",
    "    recording_utilities)\n",
    "from spinnaker_graph_front_end.utilities import SimulatorVertex\n",
    "from spinnaker_graph_front_end.utilities.data_utils import (\n",
    "    generate_system_data_region)\n",
    "\n",
    "from data_specification.enums.data_type import DataType\n",
    "\n",
    "import spinnaker_graph_front_end as front_end\n",
    "\n",
    "from spinn_front_end_common.utilities import exceptions\n",
    "import logging\n",
    "\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "\n",
    "class LatticeBasicCell(\n",
    "    SimulatorVertex, MachineDataSpecableVertex,\n",
    "    AbstractReceiveBuffersToHost,\n",
    "    AbstractProvidesNKeysForPartition\n",
    "):\n",
    "    \"\"\" A Boltzmann Lattice which represents a cell within the 2d fabric\n",
    "    \"\"\"\n",
    "    \n",
    "    PARTITION_ID = \"STATE\"\n",
    "\n",
    "    TRANSMISSION_DATA_SIZE = 2 * BYTES_PER_WORD  # has key and key\n",
    "    STATE_DATA_SIZE = BYTES_PER_WORD  # 1 or 2 based off dead or alive\n",
    "    # alive states, dead states\n",
    "    NEIGHBOUR_INITIAL_STATES_SIZE = 2 * BYTES_PER_WORD\n",
    "    RECORDING_ELEMENT_SIZE = STATE_DATA_SIZE  # A recording of the volocity\n",
    "\n",
    "    POSITION_DATA_SIZE = 2 * BYTES_PER_WORD # x position and y position\n",
    "\n",
    "    NEIGHBOUR_KEYS_SIZE = 9 * BYTES_PER_WORD  # for 8 directions and a mask\n",
    "\n",
    "    VELOCITY_SIZE = 2 * BYTES_PER_WORD  # u_x and u_y\n",
    "\n",
    "    VERTEX_INDEX_SIZE = 2 * BYTES_PER_WORD # the index of the lattice in the network\n",
    "\n",
    "    # The order of which directions are writeen to sdram\n",
    "    ORDER_OF_DIRECTIONS = [\"N\", \"W\", \"S\", \"E\", \"NW\", \"SW\", \"SE\", \"NE\"]\n",
    "    # Regions for populations\n",
    "    DATA_REGIONS = Enum(\n",
    "        value=\"DATA_REGIONS\",\n",
    "        names=[('SYSTEM', 0),\n",
    "               ('TRANSMISSIONS', 1),\n",
    "               ('POSITION', 2),\n",
    "               ('NEIGHBOUR_KEYS', 3),\n",
    "               ('VELOCITY', 4),\n",
    "               ('VERTEX_INDEX', 5),\n",
    "               ('RESULTS', 6)])\n",
    "\n",
    "    def __init__(self, label, x_position, y_position, u_x, u_y):\n",
    "        super(LatticeBasicCell, self).__init__(label, \"lattice_cell.aplx\")\n",
    "        AbstractProvidesNKeysForPartition.__init__(self)\n",
    "        # app specific data items\n",
    "        self._x_position = x_position\n",
    "        self._y_position = y_position\n",
    "        self.u_x = u_x\n",
    "        self.u_y = u_y\n",
    "        \n",
    "        self._loccation_vertices = OrderedDict()\n",
    "        for direction in self.ORDER_OF_DIRECTIONS:\n",
    "            self._loccation_vertices[direction] = None\n",
    "\n",
    "    def set_direction_vertex(self, direction, vertex):\n",
    "        \"\"\"\n",
    "        Add a vertex to the corresponding direction\n",
    "        \"\"\"\n",
    "        self._loccation_vertices[direction] = vertex\n",
    "\n",
    "    def _write_key_data(self, spec, routing_info, graph):\n",
    "        \"\"\"\n",
    "        Write the keys of its 8 neighbours\n",
    "        \"\"\"\n",
    "        spec.switch_write_focus(region=self.DATA_REGIONS.NEIGHBOUR_KEYS.value)\n",
    "        incoming_edges = graph.get_edges_ending_at_vertex_with_partition_name(self, self.PARTITION_ID)\n",
    "\n",
    "        # verify the number of edges\n",
    "        if len(incoming_edges) != 8:\n",
    "            print(incoming_edges)\n",
    "            raise exceptions.ConfigurationException(\"Should only have 8 edges\")\n",
    "\n",
    "        # get incoming edges\n",
    "        for direction in self._loccation_vertices:\n",
    "            key = routing_info.get_routing_info_from_pre_vertex(\n",
    "                self._loccation_vertices[direction],\n",
    "                self.PARTITION_ID).keys_and_masks[0].key\n",
    "            if key is not None:\n",
    "                spec.write_value(data=key)\n",
    "            else:\n",
    "                logger.warning(\"This lattice miss a edge from direction {}\".format(direction))\n",
    "                spec.write_value(data_type=DataType.INT32, data=-1)\n",
    "\n",
    "        mask = routing_info.get_routing_info_from_pre_vertex(self._loccation_vertices[\"S\"],\n",
    "                                                             self.PARTITION_ID).keys_and_masks[0].mask\n",
    "        spec.write_value(data=mask, data_type=DataType.UINT32)\n",
    "\n",
    "    @inject_items({\"data_n_time_steps\": \"DataNTimeSteps\"})\n",
    "    @overrides(\n",
    "        MachineDataSpecableVertex.generate_machine_data_specification,\n",
    "        additional_arguments={\"data_n_time_steps\"})\n",
    "    def generate_machine_data_specification(\n",
    "            self, spec, placement, machine_graph, routing_info, iptags,\n",
    "            reverse_iptags, machine_time_step, time_scale_factor,\n",
    "            data_n_time_steps):\n",
    "        # Generate the system data region for simulation .c requirements\n",
    "        generate_system_data_region(spec, self.DATA_REGIONS.SYSTEM.value,\n",
    "                                    self, machine_time_step, time_scale_factor)\n",
    "\n",
    "        # reserve memory regions for every data region\n",
    "        spec.reserve_memory_region(\n",
    "            region=self.DATA_REGIONS.TRANSMISSIONS.value,\n",
    "            size=self.TRANSMISSION_DATA_SIZE, label=\"inputs\")\n",
    "\n",
    "        spec.reserve_memory_region(\n",
    "            region=self.DATA_REGIONS.POSITION.value,\n",
    "            size=self.POSITION_DATA_SIZE, label=\"position\"\n",
    "        )\n",
    "\n",
    "        spec.reserve_memory_region(\n",
    "            region=self.DATA_REGIONS.NEIGHBOUR_KEYS.value,\n",
    "            size=self.NEIGHBOUR_KEYS_SIZE\n",
    "        )\n",
    "\n",
    "        spec.reserve_memory_region(\n",
    "            region=self.DATA_REGIONS.VELOCITY.value,\n",
    "            size=self.VELOCITY_SIZE\n",
    "        )\n",
    "\n",
    "        spec.reserve_memory_region(\n",
    "            region=self.DATA_REGIONS.VERTEX_INDEX.value,\n",
    "            size=self.VERTEX_INDEX_SIZE\n",
    "        )\n",
    "\n",
    "        spec.reserve_memory_region(\n",
    "            region=self.DATA_REGIONS.RESULTS.value,\n",
    "            size=recording_utilities.get_recording_header_size(1))\n",
    "\n",
    "        # get recorded buffered regions sorted\n",
    "        spec.switch_write_focus(self.DATA_REGIONS.RESULTS.value)\n",
    "        spec.write_array(recording_utilities.get_recording_header_array(\n",
    "            [self.RECORDING_ELEMENT_SIZE * data_n_time_steps]))\n",
    "\n",
    "        # check got right number of keys and edges going into me\n",
    "        partitions = \\\n",
    "            machine_graph.get_outgoing_edge_partitions_starting_at_vertex(self)\n",
    "        if not is_single(partitions):\n",
    "            raise ConfigurationException(\n",
    "                \"Can only handle one type of partition.\")\n",
    "\n",
    "        # check for duplicates\n",
    "        edges = list(machine_graph.get_edges_ending_at_vertex(self))\n",
    "        if len(edges) != 8:\n",
    "            raise ConfigurationException(\n",
    "                \"I've not got the right number of connections. I have {} \"\n",
    "                \"instead of 8\".format(\n",
    "                    len(machine_graph.get_edges_ending_at_vertex(self))))\n",
    "\n",
    "        for edge in edges:\n",
    "            if edge.pre_vertex == self:\n",
    "                raise ConfigurationException(\n",
    "                    \"I'm connected to myself, this is deemed an error\"\n",
    "                    \" please fix.\")\n",
    "\n",
    "        # write key needed to transmit with\n",
    "        key = routing_info.get_first_key_from_pre_vertex(\n",
    "            self, self.PARTITION_ID)\n",
    "\n",
    "        spec.switch_write_focus(\n",
    "            region=self.DATA_REGIONS.TRANSMISSIONS.value)\n",
    "        spec.write_value(0 if key is None else 1)\n",
    "        spec.write_value(0 if key is None else key)\n",
    "\n",
    "        # write POSITION data\n",
    "        spec.switch_write_focus(\n",
    "            region=self.DATA_REGIONS.POSITION.value\n",
    "        )\n",
    "        spec.write_value(int(self._x_position))\n",
    "        spec.write_value(int(self._y_position))\n",
    "\n",
    "        #write VERTEX_INDEX data. Mainly for add a random delay\n",
    "        spec.switch_write_focus(region=self.DATA_REGIONS.VERTEX_INDEX.value)\n",
    "        spec.write_value(machine_graph.vertices.index(self))\n",
    "        self.offset = generate_offset(placement.p)\n",
    "        spec.write_value(self.offset)\n",
    "\n",
    "        # write the neighbour keys and masks\n",
    "        self._write_key_data(spec, routing_info, machine_graph)\n",
    "\n",
    "        #write velocity data in two dimension, x and y\n",
    "        spec.switch_write_focus(region=self.DATA_REGIONS.VELOCITY.value)\n",
    "        spec.write_value(self.u_x, data_type=DataType.FLOAT_32)\n",
    "        spec.write_value(self.u_y, data_type=DataType.FLOAT_32)\n",
    "\n",
    "        # End-of-Spec:\n",
    "        spec.end_specification()\n",
    "\n",
    "    def get_data(self, buffer_manager, placement):\n",
    "        # for buffering output info is taken form the buffer manager\n",
    "        # get raw data, convert to list of booleans\n",
    "        raw_data, data_missing = buffer_manager.get_data_by_placement(\n",
    "            placement, 0)\n",
    "\n",
    "        # do check for missing data\n",
    "        if data_missing:\n",
    "            print(\"missing_data from ({}, {}, {}); \".format(\n",
    "                placement.x, placement.y, placement.p))\n",
    "\n",
    "        # return the data, converted to list of booleans\n",
    "        return [\n",
    "            element\n",
    "            for element in struct.unpack(\n",
    "                \"<{}f\".format(len(raw_data) // BYTES_PER_WORD), raw_data)]\n",
    "\n",
    "    @property\n",
    "    @overrides(MachineVertex.resources_required)\n",
    "    def resources_required(self):\n",
    "        \"\"\"\n",
    "        Reserve resources for data regions\n",
    "        \"\"\"\n",
    "        fixed_sdram = (SYSTEM_BYTES_REQUIREMENT + self.TRANSMISSION_DATA_SIZE +\n",
    "                       self.POSITION_DATA_SIZE +\n",
    "                       self.NEIGHBOUR_KEYS_SIZE +\n",
    "                       self.VELOCITY_SIZE +\n",
    "                       self.VERTEX_INDEX_SIZE +\n",
    "                       recording_utilities.get_recording_header_size(1) +\n",
    "                       recording_utilities.get_recording_data_constant_size(1))\n",
    "        per_timestep_sdram = self.RECORDING_ELEMENT_SIZE\n",
    "        return ResourceContainer(\n",
    "            sdram=VariableSDRAM(fixed_sdram, per_timestep_sdram))\n",
    "\n",
    "    @property\n",
    "    def x_position(self):\n",
    "        return self._x_position\n",
    "\n",
    "    @overrides(AbstractProvidesNKeysForPartition.get_n_keys_for_partition)\n",
    "    def get_n_keys_for_partition(self, partition, graph_mapper):\n",
    "        \"\"\"\n",
    "        Ask for 8 keys for every lattice for the transmission of 8 fi\n",
    "        \"\"\"\n",
    "        return 8  # for its 8 neighbours to send\n",
    "\n",
    "    @property\n",
    "    def y_position(self):\n",
    "        return self._y_position\n",
    "\n",
    "    def __repr__(self):\n",
    "        return self.label\n",
    "\n",
    "    @overrides(AbstractReceiveBuffersToHost.get_recorded_region_ids)\n",
    "    def get_recorded_region_ids(self):\n",
    "        return [0]\n",
    "\n",
    "    @overrides(AbstractReceiveBuffersToHost.get_recording_region_base_address)\n",
    "    def get_recording_region_base_address(self, txrx, placement):\n",
    "        return locate_memory_region_for_placement(\n",
    "            placement, self.DATA_REGIONS.RESULTS.value, txrx)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pacman.model.graphs.machine import MachineEdge\n",
    "\n",
    "\n",
    "class LatticeEdge(MachineEdge):\n",
    "    \"\"\"\n",
    "    Used for conjunction with a lattice\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, pre_vertex, post_vertex, compass, label=None):\n",
    "        MachineEdge.__init__(\n",
    "            self, pre_vertex, post_vertex, label=label)\n",
    "        self._compass = compass\n",
    "\n",
    "    @property\n",
    "    def compass(self):\n",
    "        return self._compass\n",
    "\n",
    "    def __str__(self):\n",
    "        return self.__repr__()\n",
    "\n",
    "    def __repr__(self):\n",
    "        return \"LatticeEdge: {}:{}\".format(self._compass, self._label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Copyright (c) 2017-2019 The University of Manchester\n",
    "#\n",
    "# This program is free software: you can redistribute it and/or modify\n",
    "# it under the terms of the GNU General Public License as published by\n",
    "# the Free Software Foundation, either version 3 of the License, or\n",
    "# (at your option) any later version.\n",
    "#\n",
    "# This program is distributed in the hope that it will be useful,\n",
    "# but WITHOUT ANY WARRANTY; without even the implied warranty of\n",
    "# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the\n",
    "# GNU General Public License for more details.\n",
    "#\n",
    "# You should have received a copy of the GNU General Public License\n",
    "# along with this program.  If not, see <http://www.gnu.org/licenses/>.\n",
    "import math\n",
    "import os\n",
    "# from pacman.model.graphs.machine import MachineEdge\n",
    "\n",
    "import spinnaker_graph_front_end as front_end\n",
    "\n",
    "# 12000\n",
    "\n",
    "MAX_X_SIZE_OF_FABRIC = 128\n",
    "MAX_Y_SIZE_OF_FABRIC = 128\n",
    "n_chips = (MAX_X_SIZE_OF_FABRIC * MAX_Y_SIZE_OF_FABRIC) // 10\n",
    "\n",
    "ex = [0, 1, 0, -1, 0, 1, -1, -1, 1]\n",
    "ey = [0, 0, 1, 0, -1, 1, 1, -1, -1]\n",
    "\n",
    "\n",
    "def initVelocity(x_pos, y_pos):\n",
    "    \"\"\"\n",
    "    Init the velocity u, v = (u_x, u_y)\n",
    "  \n",
    "    ydim = xdim = N = 128\n",
    "    K = 30 / N\n",
    "    delta = 0.05\n",
    "  \n",
    "    u = tanh( K (y - 0.25 * ydim) ) for y <= 0.5 * ydim \n",
    "    u = U_0 tanh( K (0.75 * ydim - y) ) for y >  0.5 * ydim\n",
    "    v = delta sin( 2pi *x / xdim )\n",
    "    \"\"\"\n",
    "    U_0 = 0.01\n",
    "    K = 30.0\n",
    "    delta = 0.05\n",
    "    x_temp = 1.0 * (x_pos) / MAX_X_SIZE_OF_FABRIC\n",
    "    y_temp = 1.0 * (y_pos) / MAX_Y_SIZE_OF_FABRIC\n",
    "    if y_temp <= 0.5:\n",
    "        u_x = U_0 * math.tanh(K * (y_temp - 0.25))\n",
    "    else:\n",
    "        u_x = U_0 * math.tanh(K * (0.75 - y_temp))\n",
    "    u_y = U_0 * delta * math.sin(2 * math.pi * (x_temp + 0.25))\n",
    "    return u_x, u_y\n",
    "\n",
    "\n",
    "# set up the front end and ask for the detected machines dimensions\n",
    "front_end.setup(\n",
    "    n_chips_required=n_chips, model_binary_folder=os.path.dirname(os.path.abspath(\"__file__\")), machine_time_step=time_step,time_scale_factor=time_scale_factor)\n",
    "\n",
    "# figure out if machine can handle simulation\n",
    "cores = front_end.get_number_of_available_cores_on_machine()\n",
    "\n",
    "# chech if there is enough cores\n",
    "if cores <= (MAX_X_SIZE_OF_FABRIC * MAX_Y_SIZE_OF_FABRIC):\n",
    "    raise KeyError(\"Don't have enough cores to run simulation\")\n",
    "\n",
    "# contain the vertices for the connection aspect\n",
    "vertices = [\n",
    "    [None for _ in range(MAX_X_SIZE_OF_FABRIC)]\n",
    "    for _ in range(MAX_Y_SIZE_OF_FABRIC)]\n",
    "\n",
    "# build vertices\n",
    "for x in range(0, MAX_X_SIZE_OF_FABRIC):\n",
    "    for y in range(0, MAX_Y_SIZE_OF_FABRIC):\n",
    "        u_x, u_y = initVelocity(x, y)\n",
    "        vert = LatticeBasicCell(\n",
    "            \"cell{}\".format((x * MAX_X_SIZE_OF_FABRIC) + y),\n",
    "            x, y, u_x, u_y)\n",
    "        vertices[x][y] = vert\n",
    "        front_end.add_machine_vertex_instance(vert)\n",
    "\n",
    "# build edges\n",
    "for x in range(0, MAX_X_SIZE_OF_FABRIC):\n",
    "    for y in range(0, MAX_Y_SIZE_OF_FABRIC):\n",
    "        #   diraction = [\"me\", \"n\", \"w\", \"s\", \"e\", \"nw\", \"sw\", \"se\", \"ne\"]\n",
    "        positions = [\n",
    "            (x, (y + 1) % MAX_Y_SIZE_OF_FABRIC, \"E\"),\n",
    "            ((x + 1) % MAX_X_SIZE_OF_FABRIC,\n",
    "             (y + 1) % MAX_Y_SIZE_OF_FABRIC, \"SE\"),\n",
    "            ((x + 1) % MAX_X_SIZE_OF_FABRIC, y, \"S\"),\n",
    "            ((x + 1) % MAX_X_SIZE_OF_FABRIC,\n",
    "             (y - 1) % MAX_Y_SIZE_OF_FABRIC, \"SW\"),\n",
    "            (x, (y - 1) % MAX_Y_SIZE_OF_FABRIC, \"W\"),\n",
    "            ((x - 1) % MAX_X_SIZE_OF_FABRIC,\n",
    "             (y - 1) % MAX_Y_SIZE_OF_FABRIC, \"NW\"),\n",
    "            ((x - 1) % MAX_X_SIZE_OF_FABRIC, y, \"N\"),\n",
    "            ((x - 1) % MAX_X_SIZE_OF_FABRIC,\n",
    "             (y + 1) % MAX_Y_SIZE_OF_FABRIC, \"NE\")]\n",
    "\n",
    "         # build edges for each direction for this vertex\n",
    "        for (dest_x, dest_y, compass) in positions:\n",
    "            front_end.add_machine_edge_instance(LatticeEdge( vertices[x][y], vertices[dest_x][dest_y],compass, \"edge between {} and {}\".format(vertices[x][y], vertices[dest_x][dest_y])), LatticeBasicCell.PARTITION_ID)\n",
    "            vertices[x][y].set_direction_vertex(direction=compass, vertex=vertices[dest_x][dest_y])\n",
    "\n",
    "# run the simulation\n",
    "front_end.run(runtime)\n",
    "\n",
    "# get recorded data\n",
    "recorded_data = dict()\n",
    "\n",
    "# if not front_end.use_virtual_machine():\n",
    "buffer_manager = front_end.buffer_manager()\n",
    "\n",
    "# get the data per vertex\n",
    "for x in range(0, MAX_X_SIZE_OF_FABRIC):\n",
    "    for y in range(0, MAX_Y_SIZE_OF_FABRIC):\n",
    "        recorded_data[x, y] = vertices[x][y].get_data(\n",
    "            front_end.buffer_manager(),\n",
    "            front_end.placements().get_placement_of_vertex(\n",
    "                vertices[x][y]))\n",
    "# clear the machine\n",
    "front_end.stop()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(recorded_data[1,1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# visualise it in text form (bad but no vis this time)\n",
    "u_x_output = collections.defaultdict(dict)\n",
    "u_y_output = collections.defaultdict(dict)\n",
    "for i in range(0, MAX_X_SIZE_OF_FABRIC):\n",
    "    for j in range(0, MAX_Y_SIZE_OF_FABRIC):\n",
    "        u_x_output[i][j] = recorded_data[i, j][23998]\n",
    "        u_y_output[i][j] = recorded_data[i, j][23999]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# calculate the voriticity\n",
    "voriticity = collections.defaultdict(dict)\n",
    "\n",
    "for i in range(0, MAX_X_SIZE_OF_FABRIC):\n",
    "    for j in range(0, MAX_Y_SIZE_OF_FABRIC): \n",
    "        voriticity[i][j] = 0.5 * (u_y_output[(i + 1) % MAX_X_SIZE_OF_FABRIC][j] - u_y_output[(i - 1)%MAX_X_SIZE_OF_FABRIC][j]) + 0.5 * (u_x_output[i][(j + 1)%MAX_Y_SIZE_OF_FABRIC] - u_x_output[i][(j - 1) % MAX_Y_SIZE_OF_FABRIC])\n",
    "voriticity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Visulize the voriticity with contour\n",
    "data = pd.DataFrame.from_dict(voriticity).T\n",
    "X = np.linspace(0,127,128)\n",
    "Y = np.linspace(0,127,128)\n",
    "def f(X,Y):\n",
    "    return (data.iloc[X,Y])\n",
    "\n",
    "fig = plt.figure()\n",
    "plt.title('128 * 128 contour of Vorticity')\n",
    "ax = plt.subplot(1,1,1)\n",
    "plt.contour(X, Y, f(X,Y),20, colors='k', linewidths=(0.7,))\n",
    "plt.contourf(X, Y, f(X,Y),20, alpha=0.25, colors='black', linewidths=0.7)\n",
    "ax.set_aspect('equal')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# save the file\n",
    "data.to_csv(r'Vorticity_128*128.csv', index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "         "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "sPyNNaker",
   "language": "python",
   "name": "spynnaker"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
